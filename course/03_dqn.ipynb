{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../qlearning/car.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning\n",
    "\n",
    "In this lesson, we’ll dive into Q-Learning, a popular value-based reinforcement learning algorithm. Unlike policy-based methods (like Policy Gradients), Q-Learning aims to learn the optimal value of state-action pairs to derive a policy indirectly.\n",
    "\n",
    "Q-Learning is an off-policy algorithm, meaning that it learns the optimal policy regardless of the actions taken by the agent during training. The goal is to estimate the Q-value function, which represents the expected future rewards when taking an action in a given state and following the optimal policy thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-value, denoted as Q(s,a)Q(s,a), is a measure of the expected cumulative reward the agent can obtain starting from state ss, taking action aa, and then following the optimal policy. Formally, it can be expressed as:\n",
    "\n",
    "$$Q(s,a)=E[Gt∣st=s,at=a]$$\n",
    "\n",
    "\n",
    "Where $Gt$​ is the total discounted return from time step $t$ onward.\n",
    "\n",
    "The central idea of Q-Learning is to iteratively update the Q-values using the Bellman equation, which we’ll explore in more detail below:\n",
    "$$Q(s,a)←Q(s,a)+α[r+γmax⁡a′Q(s′,a′)−Q(s,a)]$$\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "- $α$ is the learning rate, controlling how much we update the Q-value,\n",
    "- $r$ is the reward received after taking action aa in state ss,\n",
    "- $s$ is the next state,\n",
    "- $γ$ is the discount factor (same as in policy gradients), balancing immediate vs. future rewards,\n",
    "- $maxa′​Q(s′,a′)$ is the maximum Q-value of the next state s′s′, assuming the best action is taken in that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Temporal Difference (TD) Learning is a key component of Q-Learning. It allows an agent to learn from raw experience without waiting for the episode to finish. TD learning updates Q-values incrementally after each action, by bootstrapping from the estimated Q-values of the next state. Unlike Monte Carlo method that we saw in the previous chapter, which wait until the end of an episode to update, TD learning updates at every step, allowing for more immediate adjustments.\n",
    "\n",
    "In TD learning, we estimate the Q-value at each step by combining the immediate reward and the estimated value of the next state-action pair. This estimation is known as the TD target:\n",
    "$$TD target=r+γmax⁡a′Q(s′,a′)$$\n",
    "\n",
    "\n",
    "The TD error is the difference between the TD target and the current Q-value estimate:\n",
    "$$TD error=[r+γmax⁡a′Q(s′,a′)−Q(s,a)]$$\n",
    "\n",
    "\n",
    "This error measures how far off the current Q-value is from the expected future rewards. The Q-value is then updated using the TD error as follows:\n",
    "$$Q(s,a)←Q(s,a)+α TD error$$\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "- $α$ is the learning rate, determining how much the Q-value is adjusted in response to the TD error,\n",
    "- $γ$ is the discount factor, determining how much future rewards are valued.\n",
    "\n",
    "In contrast to Monte Carlo methods, which wait until the end of an episode to update values, TD learning updates the Q-value after every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"../images/flag.png\" alt=\"Description of the image\" style=\"width: 100px; height: auto;\">\n",
    "    <h2 style=\"margin-right: 20px;\">The Bellman Equation</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman equation is a recursive formula that expresses the relationship between the value of a state-action pair and the value of the subsequent states. For Q-Learning, the Bellman equation is used to update the Q-values based on the reward from the current step and the expected future rewards from the next state.\n",
    "\n",
    "The Bellman equation for Q-values is:\n",
    "$$Q(s,a)=E[r+γmax⁡a′Q(s′,a′)∣s,a]$$\n",
    "\n",
    "\n",
    "This equation shows that the Q-value of a state-action pair $(s,a)$ is equal to the immediate reward $r$ plus the discounted maximum Q-value of the next state $s′$, given that the best action a′a′ is taken in that state.\n",
    "\n",
    "When the agent updates its Q-values during training, it uses this Bellman equation in the form of an update rule:\n",
    "\n",
    "$$Q(s,a)←Q(s,a)+α[r+γa′max​Q(s′,a′)−Q(s,a)]$$\n",
    "\n",
    "Here, the term $r+γmaxa′​Q(s′,a′)$ represents the TD target (the updated estimate of the Q-value), and the difference between this and the current estimate Q(s,a)Q(s,a) is the TD error. This error drives the update, gradually improving the Q-values over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-Learning algorithm works by interacting with the environment, updating Q-values based on experience, and gradually converging to the optimal policy. The process typically follows these steps:\n",
    "\n",
    "1. Initialize Q-values: Start by initializing the Q-table (or function) for all state-action pairs arbitrarily (often to zeros).\n",
    "\n",
    "2. For each episode:\n",
    "    - Observe the current state ss.\n",
    "    - Choose an action aa using an exploration strategy like epsilon-greedy:\n",
    "        With probability $ϵ$, choose a random action (exploration),\n",
    "        Otherwise, choose the action with the highest Q-value (exploitation).\n",
    "    - Execute action aa, observe the reward $r$, and the next state $s′$.\n",
    "    - Update the Q-value for $(s,a)$ using the Bellman equation.\n",
    "    - Set the current state to $s′$ and repeat until the episode ends.\n",
    "\n",
    "3. Repeat for many episodes: Over time, the Q-values should converge to the optimal values, and the agent will learn the best policy (the set of actions that maximizes long-term rewards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q-Learning, a common exploration strategy is the epsilon-greedy approach. This strategy balances exploration (trying new actions) and exploitation (choosing the best-known actions). It works as follows:\n",
    "\n",
    "- With a probability $ϵ$, choose a random action (exploration),\n",
    "- With a probability $1−ϵ$, choose the action with the highest Q-value (exploitation).\n",
    "\n",
    "This ensures that the agent explores the environment sufficiently, especially early in training, while gradually shifting towards exploiting its learned policy as ϵϵ decays over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay memory is a technique we will be using, where the agent stores past experiences in a memory buffer and reuses them for training. Instead of learning only from consecutive steps, the agent randomly samples past experiences to break correlations and improve learning stability. This allows the agent to learn from a diverse set of experiences and reinforce important information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class QRace:\n",
    "\n",
    "    def training_race(self, car, episodes):\n",
    "\n",
    "        for episode in range(1, episodes + 1):\n",
    "\n",
    "            current_state = car.get_data()\n",
    "\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                \n",
    "                action = car.action_train(current_state)\n",
    "                new_state, reward, done = self.step(car)\n",
    "                episode_reward += reward\n",
    "\n",
    "                current_state = new_state\n",
    "                car.update_replay_memory(current_state, action, reward, new_state, done)\n",
    "\n",
    "                loss = car.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The policy update or agent's training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class QCar:\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        mini_batch = random.sample(self.replay_memory, self.mini_batch_size)\n",
    "\n",
    "        states, actions, rewards, new_states, dones = zip(*mini_batch)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)\n",
    "        new_states = torch.tensor(np.array(new_states), dtype=torch.float32).to(\n",
    "            self.device\n",
    "        )\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Compute Q values\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Use the online network to select actions for the next state\n",
    "        next_actions = self.model(new_states).argmax(1).unsqueeze(1)\n",
    "\n",
    "        next_q_values = self.target_model(new_states).gather(1, next_actions).squeeze(1)\n",
    "        target_q_values = rewards + self.discount_factor * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # td_errors = (target_q_values - current_q_values).detach().cpu().numpy()\n",
    "        # self.replay_buffer.update_priorities(indices, td_errors)\n",
    "\n",
    "        self.update_target_network()\n",
    "        self.epsilon_decay()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[INSERT VIDEO OF THE ACTUAL TRAINING]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
